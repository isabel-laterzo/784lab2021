---
title: "Lab 12: Multilevel Linear Models"
author: "Multiple authors"
date: "April 14, 2021"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, fig.align = 'center', message = F
                      ) # code chunks included by default
library(xtable)
library(knitr)
library(lme4)
library(tidyverse)
```

## A Note on Fixed vs. Random Effects

There are a staggering number of different names for these models, with different disciplines using different terminology. In the language used in this course, *fixed effects* are typically non-random intercepts (typically implemented with group dummies), *random effects* are random variables (which are typically slopes *or* intercepts) drawn from a common distribution, and multilevel/mixed-effects models are models that include any combination of these components.

Unfortunately, one of the most widely used packages in R for multilevel models uses some confusing terminology. The `lme4` package uses the following terms:

- fixed effect: any non-random parameter (not just intercepts)
- random effect: random RHS term, drawn from a distribution

## Fixed Effect Models

Load the radon data from http://www.stat.columbia.edu/~gelman/arm/examples/radon/srrs2.dat, and log transform the radon `activity` variable (add a 1 to avoid issues with taking the log of zero). Drop any observations with a value of 9 for `floor`, as this is a missing data code and treating it as a value will bias our estimates. Use `state2` as the state variable for all subsequent examples, so we will transform it to treat it as a factor.

```{r}
sr <- read.delim('http://www.stat.columbia.edu/~gelman/arm/examples/radon/srrs2.dat', sep = ',')
sr$radon <- log(sr$activity+1)
sr <- sr[which(sr$floor < 9), ]
sr$state2 <- as.factor(sr$state2)
```


Run a standard linear model (i.e. the complete pooling model) explaining the level of `radon` as a function of `floor`, and a model with fixed effects, using states as the group. The lm function creates these dummies (or treatment-coded contrasts) automatically for us.


```{r}
pooled <- lm(radon ~ floor, data = sr)
fix_int <- lm(radon ~ floor + state2, data = sr)
```


We can also estimate a fixed effects model with varying *slopes* instead of intercepts, although interpreting and presenting these results is not the easiest task. Include state dummies like before (by simply including the `state2` variable on the RHS), but interact them with `floor` this time. This produces a model with varying slope and intercept fixed effects; this is closest to the completely separate regression approach (a.k.a no-pooling) (as the conditional variance of $y$ given $x$ is still common). But it is estimating effects using subsets of observations in each state (thus the no-pooling name):

```{r}
fix_slope <- lm(radon ~ floor * state2, data = sr)
```


We could also omit the group dummies as a separate predictor, yielding a model with varying slope fixed effects. The figure below shows the regression lines for each model. In the next section we'll see how to create these plots for random effects models.



```{r, fig.width = 10, fig.height = 9, echo = F, fig.cap = 'd'}
sr$fit <- predict(fix_int)
fe_i <- ggplot(data = sr, aes(x = floor, y = radon)) +
  facet_wrap(~state2)+
  geom_point(alpha = .25, position = 'jitter', size = .75) + 
  geom_line(aes(y = fit), col="red") +
  labs(title = 'Varying Intercept') +
  theme_bw() 

fe_slopes_noint <- lm(radon ~ floor * state2 - state2, data = sr)
sr$fit <- predict(fe_slopes_noint)
fe_s <- ggplot(data = sr, aes(x = floor, y = radon)) +
  facet_wrap(~state2)+
  geom_point(alpha = .25, position = 'jitter', size = .75) + 
  geom_line(aes(y = fit), col="red") +
  labs(title = 'Varying Slope') +
  theme_bw()

fe_slopes <- lm(radon ~ floor * state2, data = sr)
sr$fit <- predict(fe_slopes)
fe_si <- ggplot(data = sr, aes(x = floor, y = radon)) +
  facet_wrap(~state2)+
  geom_point(alpha = .25, position = 'jitter', size = .75) + 
  geom_line(aes(y = fit), col="red") +
  labs(title = 'Varying Slope and Intercept') +
  theme_bw() 

gridExtra::grid.arrange(fe_i, fe_s, fe_si, nrow = 3)
```



## Random Effects Models


If we want to estimate a varying intercept model by county, there are some difficulties we run into. There are `r length(unique(sr$county))` counties in the data, so this model would be very difficult to read and work with. As an alternative, we might want to estimate a random effects model. Doing so would let us to describe the distribution that the varying intercepts are drawn from, allowing us to better understand what a random intercept of -.72 means relative to the mean of the distributions the intercepts are drawn from, and leveraging similarities across the `r length(unique(sr$county))` groups. We can use the `lmer()` function in the package `lme4`, to accomplish this. This function produces a model object of class `lmerMod`. Estimate a random effects model with radon levels explained by floor and varying intercept by state (it's much easier to understand how these models work with `r length(unique(sr$state2))` groups instead of `r length(unique(sr$county))`).

```{r, message = F}
library(lme4)
ran_int_lme <- lmer(radon ~ floor + (1 | state2), data = sr)
```

Note the formula, which looks different from the regular `lm` formula. When we use `lmer` (and also `glmer` and many different functions that allow us to estimate random effects, as they have adopted this notation as well), we place the group identifier (always a factor, although if we supply `lmer` a numeric variable, it will coerce it to be a factor) after the vertical bar `|`. This model is a varying intercepts model that varies *by* state.

This model assumes that there is a different baseline value for `radon` based on differences in `state2`. When you call the model object, or use `summary()` on it, you'll get the usual estimates and standard errors for your non-random effects (your regular coefficients), but only the variance and standard deviation for your random effects (the varying intercepts). Since we assume that these intercepts are all drawn from the same distribution, we can describe the set of intercepts using standard deviation or variance. If we want to examine what these intercepts actually are, we can use the `coef()` function. Using `coef()` on a `lmerMod` is convenient, because it will automatically combine any non-random and random effects, and then report the resulting coefficients by group membership.


```{r}
coef(ran_int_lme)$state2
```

Note that the intercept is different for each state, but that the slope of floor stays the same. 

If we want to extract the individual non-random and random effects, we can do so using `fixef()` and `ranef()`, respectively.
```{r}
(ranef(ran_int_lme)$state2)
fixef(ran_int_lme)
```

`fixef` returns the average effect for a random effect, so it serves as the baseline for the deviations specified by `ranef`. `ranef` can be thought of as offsets w.r.t that average effect. We can therefore combine the fixed and random effects to recover the reported effects returned by `coef()`. IN this case, that means that the overall intercept (i.e. `fixef(ran_int_lme)[1]`) is the mean of the distribution of random intercepts.

Let's try this for Missouri, comparing the effect of both the intercept and `floor`.

```{r, results = 'hold'}
## Mean vs output of coef
fixef(ran_int_lme)[1] == coef(ran_int_lme)$state2[6,1]
## Mean plus random offser vs. output of coef
fixef(ran_int_lme)[1] + ranef(ran_int_lme)$state2[6,1] == coef(ran_int_lme)$state2[6, 1]
## non-random term vs. output of coef
fixef(ran_int_lme)[2] == coef(ran_int_lme)$state2[6, 2]
```

This is a lot of words to describe what's happening in a random effects varying intercept model, but sometimes it's easier to understand with a picture. Use the `predict()` function on our `lmerMod` object to get fitted values and then plot `radon` and `floor` values with the regression line for each state.


```{r, fig.width = 10}
sr$fit <- predict(ran_int_lme)
ggplot(data = sr, aes(x = floor, y = radon)) +
  facet_wrap(~state2)+
  geom_point(alpha = .25, position = 'jitter', size = .75) + 
  geom_line(aes(y = fit)) +
  theme_bw() 
```

We can also access the standard errors of the fixed and random effects using `se.fixef()` and `se.ranef()` in the `arm` package. The `REextract()` function in `merTools` allows us to easily extract both the estimates and standard errors of random effects.

```{r}
library(arm)
se.ranef(ran_int_lme)$state2
se.fixef(ran_int_lme)
library(merTools)
REextract(ran_int_lme)
```


`lme4` can also specify varying *slope* random effects models. In these models, the effect of a predictor is dependent on group membership. Write a model where `radon` is explained by `floor`, but with a coefficient that varies by `state2`. To do so, add a `+ floor` to the right of the `|` in the random effects term:

```{r}
mlm <- lmer(radon ~ floor + (1 + floor | state2), data = sr)
```


As with the models before, we can access the non-random, random, and total effects separately. Compare the `floor` value for Indiana in the random effect component to the `floor` coefficient for Indiana: see how one is a offset version of the other?


```{r}
coef(mlm)$state2
fixef(mlm)
ranef(mlm)$state2
(fixef(mlm)[2] + ranef(mlm)$state2[2, 2]) == coef(mlm)$state2[2, 2]
```

Just like we can use plots to visually explore the effect of random intercepts, we can do the same with random slopes. Use the `predict()` command again to create a scatter plot of radon levels against measurement floor, and include the regression line for each state.

```{r, fig.width = 10}
sr$fit<- predict(mlm)
slope_plot <- ggplot(data = sr, aes(x = floor, y = radon)) +
  facet_wrap(~state2)+
  geom_point(alpha = .25, position = 'jitter', size = .75) +
  geom_line(aes(y = fit), col="red") +
  theme_bw() 
slope_plot
```

We can also specify a random effects model without a varying intercept term. To specify a model that explains the radon level as a function of `floor` with a varying slope by `state2`, but no random intercept by the same, simply replace the `1` in the random effects term with a `-1`.  Look at the summary of the model object and notice that there is no longer an intercept listed under the random effects. This means that observations in each group will have their own slope, but use the overall intercept. Plotting this model, the random intercepts and random slopes model, and the complete-pooling model clearly illustrates the difference between them:

```{r, fig.width = 10, fig.height = 9}
mlm <- lmer(radon ~ (floor - 1 | state2), data = sr)
sr$fit <- predict(mlm)
slope_plot_noint <- ggplot(data = sr, aes(x = floor, y = radon))+
  facet_wrap(~state2)+
  geom_point(alpha = .25, position = 'jitter', size = .75) +
  geom_line(aes(y = fit), col = "red") +
  labs(title = 'Shared Intercept, Random Slopes', x = 'floor') +
  theme_bw() 
slope_plot <- slope_plot +
  labs(title = 'Random Intercepts, Random Slopes', x = 'floor') +
  theme(plot.title = element_text(hjust = 0.5))
sr$fit <- predict(lm(radon ~ floor, data = sr))
slope_plot_pool <- ggplot(data = sr, aes(x = floor, y = radon)) +
  facet_wrap(~state2)+
  geom_point(alpha = .25, position = 'jitter', size = .75) +
  geom_line(aes(y = fit), color = 'red') +
  labs(title = 'Shared Intercept, Shared Slope', x = 'floor') +
  theme_bw() 

## arrange in order of increasing similarity
gridExtra::grid.arrange(slope_plot, slope_plot_noint, slope_plot_pool, nrow = 3)
```

# Group Level Predictors 
Adding group level predictors for the intercept is straightforward. We just include predictors that vary only at the group level in the formula as non-random effects - we are making no assumptions that they are drawn from any distribution and we estimate them as usual (although we are *really* simultaneously estimating an equation for the intercept). 

To illustrate, lets change the grouping variable to county. Load the data file of county level info from http://www.stat.columbia.edu/~gelman/arm/examples/radon/cty.dat.

Before we can merge the county-level data onto the individual-level data, we need to have common identifiers. These data are from the US, so they use FIPS codes. The data only have separate county and state level FIPS codes. The formula for full FIPS codes is $ (1000 \cdot \text{state}) + \text{county} $. Create full FIPS codes in both the individual and county level data, then merge them together.

```{r}
cty <- read.delim('http://www.stat.columbia.edu/~gelman/arm/examples/radon/cty.dat', sep = ',')
sr$fips <- sr$stfips * 1000 + sr$cntyfips
cty$fips <- cty$stfips * 1000 + cty$ctfips
ml_data <- merge(sr, cty)
```

We will use the variable `Uppm`, which measures uranium concentration in the soil in each county.
```{r}
m4 <- lmer(radon ~ floor + Uppm + (floor|cty), data = ml_data)
summary(m4)
```

```{r}
ranef(m4)$cty[1:6,]
fixef(m4)
coef(m4)$cty[1:6, ]
```
You just need to remember that the coefficient associated with `Uppm` substantively affects the the random *intercept*. We can  use all of the above to get our linear predictor, which we will do for Adair county:
```{r}
#get Uppm value for Adair county
Uppm_ADAIR <- ml_data$Uppm[which(ml_data$cty == "ADAIR")] %>% unique

#intercept for specific county:
fixef(m4)[1] + #mean of random intercept  
  ranef(m4)$cty[1, 1] + #random deviation from that average for Adair cty
  fixef(m4)[3] * Uppm_ADAIR #effect of Uppm multiplied by Uppm for Adair

#effect of floor for Adair
fixef(m4)[2] + ranef(m4)$cty[1, 2]
```

At the same time, note that we will still get the correct linear predictor for observation *i* if we just plug in the values for observation *i* into result of `coef()`.

This is not quite so straightforward when we have group level *slope* predictors, although once you wrap your head around it, it sparks joy. We will use the fact that multilevel models are really just interactions to our advantage (we did this above, really, but it becomes more noticeable here). 

Long story short: we have to interact the variable whose coefficient we want to vary by a group with the group level predictor. Thus, if we thought that the impact of floor was also affected, for some reason, by the uranium level in the soil in a certain county, we would have to do the following:
```{r}
m5 <- lmer(radon ~ floor + Uppm + floor:Uppm + (1+floor|cty), 
           data = ml_data)
summary(m5)
```

We could also do `floor*Uppm` and get the same effect, just like with `lm`, but this is easier for the sake of illustration. 

The coefficient on `Uppm` can be interpreted the same as above: it is the effect of `Uppm` on the coefficient for the intercept. 
The coefficient on `floor:Uppm` represents, then, the effect of `Uppm` on the coefficient for floor.

Thus, we can calculate the "true" coefficients in the following way, once again for Adair Country:
```{r}
#intercept (same as above)
fixef(m5)[1] + #average intercept  
  ranef(m5)$cty[1, 1] + #random deviation from that average for Adair cty
  fixef(m5)[3] * Uppm_ADAIR #effect of Uppm multiplied by Uppm for Adair

#floor
fixef(m5)[2] + #average effect
  ranef(m5)$cty[1, 2] + #deviation for Adair
  fixef(m5)[4] * Uppm_ADAIR #effect of Uppm for floor multiplied by Uppm for Adair

coef(m5)$cty[1,]
```
Why does this work? How would we calculate the linear predictor for `floop:Uppm`? We would do `floor:Uppm` X value of floor X value of Uppm. So, we can just factor out the value of floor, which varies on the individual, not group level, like Uppm.   


## Lab Assignment

Use the `WDI` package to download GDP per capita (NY.GDP.PCAP.KD) and gross domestic savings (NY.GNS.ICTR.ZS) data for the Nordic countries from 1960-2016. Fit varying intercept, varying slope, and varying intercept and slope random effects models that explain GDP as a function of savings, with country as the group. Produce plots illustrating the regression lines for each country in each model.
```{r}

```


